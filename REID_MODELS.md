# Advanced Re-ID Models for BoT-SORT Tracking

This document describes the advanced Re-ID (Re-identification) backbone models available for enhancing BoT-SORT tracking accuracy in MiVOLO.

## Quick Start

1. **List available models:**
   ```bash
   python setup_reid_models.py --list
   ```

2. **Get recommendations for your use case:**
   ```bash
   python setup_reid_models.py --recommend balanced  # Default recommendation
   python setup_reid_models.py --recommend speed     # For real-time applications
   python setup_reid_models.py --recommend accuracy  # For maximum accuracy
   ```

3. **Download and configure a model:**
   ```bash
   # Download the recommended OSNet model
   python setup_reid_models.py --download osnet_x1_0_msmt17.pt --use osnet_x1_0_msmt17.pt
   ```

4. **Use with MiVOLO:**
   ```bash
   python demo.py --input video.mp4 --output output/ \
     --detector-weights models/yolov8x_person_face.pt \
     --checkpoint models/mivolo_imbd.pth.tar \
     --tracker botsort.yaml \
     --confidence-based \
     --with-persons --draw
   ```

## Model Categories

### 1. OSNet Models (Recommended)

**Best for:** Balanced speed and accuracy, general-purpose tracking

- **osnet_x1_0_msmt17.pt** â­ **Recommended**
  - Speed: Fast | Accuracy: High | Embedding: 512D
  - Best overall balance for most applications
  - Memory efficient with good discriminative power

- **osnet_x0_75_msmt17.pt**
  - Speed: Very Fast | Accuracy: High | Embedding: 512D  
  - Slightly faster than x1.0 with minimal accuracy loss
  - Good for resource-constrained environments

- **osnet_x0_5_msmt17.pt**
  - Speed: Ultra Fast | Accuracy: Medium-High | Embedding: 512D
  - Fastest OSNet variant for real-time applications
  - Acceptable trade-off for high-FPS requirements

### 2. ResNeSt Models (High Accuracy)

**Best for:** Maximum accuracy when compute resources are available

- **resnest50_market1501.pt**
  - Speed: Medium | Accuracy: Very High | Embedding: 2048D
  - ResNeSt backbone with split-attention mechanism
  - Excellent for challenging scenarios with similar appearances

- **resnest101_msmt17.pt**
  - Speed: Slow | Accuracy: Exceptional | Embedding: 2048D
  - Largest model with highest accuracy
  - Use when accuracy is more important than speed

### 3. CLIP-based Models (State-of-the-art)

**Best for:** Cross-modal understanding and exceptional accuracy

- **clip_vit_b16_market1501.pt**
  - Speed: Medium | Accuracy: Very High | Embedding: 768D
  - Vision Transformer backbone with CLIP pre-training
  - Excellent generalization to different domains

- **clip_vit_l14_msmt17.pt** â­ **Best Accuracy**
  - Speed: Slow | Accuracy: State-of-the-art | Embedding: 1024D
  - Largest CLIP model for maximum discriminative power
  - Best choice when accuracy is paramount

### 4. MobileNet Models (Edge Deployment)

**Best for:** Mobile devices and edge computing

- **mobilenetv2_x1_4_msmt17.pt**
  - Speed: Ultra Fast | Accuracy: Medium | Embedding: 1280D
  - Optimized for mobile deployment
  - Good choice for real-time mobile applications

### 5. ConvNeXt Models (Modern CNN)

**Best for:** Modern CNN architecture with transformer-like performance

- **convnext_small_msmt17.pt**
  - Speed: Fast | Accuracy: High | Embedding: 768D
  - Modern CNN design with competitive performance
  - Good balance of efficiency and accuracy

- **convnext_base_market1501.pt**
  - Speed: Medium | Accuracy: Very High | Embedding: 1024D
  - Larger ConvNeXt for better feature representation
  - Excellent choice for high-quality tracking

## Performance Comparison

| Model Category | Speed | Accuracy | Memory | Use Case |
|---|---|---|---|---|
| OSNet x0.5 | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | ðŸ”‹ðŸ”‹ | Real-time, low latency |
| OSNet x0.75 | âš¡âš¡âš¡âš¡ | â­â­â­â­ | ðŸ”‹ðŸ”‹ | Balanced performance |
| OSNet x1.0 | âš¡âš¡âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ | **Recommended default** |
| ConvNeXt Small | âš¡âš¡âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ | Modern architecture |
| CLIP ViT-B/16 | âš¡âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ | Cross-modal features |
| ResNeSt-50 | âš¡âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ | High accuracy |
| ConvNeXt Base | âš¡âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ | Better feature quality |
| CLIP ViT-L/14 | âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ | **Maximum accuracy** |
| ResNeSt-101 | âš¡ | â­â­â­â­â­ | ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ðŸ”‹ | Research/offline use |
| MobileNetV2 | âš¡âš¡âš¡âš¡âš¡ | â­â­â­ | ðŸ”‹ | Mobile/edge devices |

## Configuration Details

The `botsort.yaml` configuration includes several advanced parameters for Re-ID optimization:

```yaml
# Core ReID settings
with_reid: true
proximity_thresh: 0.2     # IoU threshold for ReID matching
appearance_thresh: 0.25   # Appearance similarity threshold

# Model selection (choose one)
model: osnet_x1_0_msmt17.pt  # Your chosen model

# Advanced parameters
alpha: 0.9                # EMA update rate (0.9 = more stable)
nn_budget: 30            # Max tracklet history length
max_age: 50              # Max frames to keep lost tracks
embedding_dim: 512       # Feature dimension (model-dependent)
normalize_embeddings: true
temperature: 0.07        # Cosine similarity temperature
min_confidence: 0.3      # Min detection confidence for ReID
reid_thresh: 0.6         # ReID similarity threshold
```

## Use Case Recommendations

### Real-time Applications (>20 FPS)
```bash
python setup_reid_models.py --download osnet_x0_5_msmt17.pt --use osnet_x0_5_msmt17.pt
```

### Balanced Performance (Default)
```bash
python setup_reid_models.py --download osnet_x1_0_msmt17.pt --use osnet_x1_0_msmt17.pt
```

### Maximum Accuracy (Offline Processing)
```bash
python setup_reid_models.py --download clip_vit_l14_msmt17.pt --use clip_vit_l14_msmt17.pt
```

### Mobile/Edge Deployment
```bash
python setup_reid_models.py --download mobilenetv2_x1_4_msmt17.pt --use mobilenetv2_x1_4_msmt17.pt
```

## Technical Benefits

### Why Advanced Re-ID Models Matter

1. **Better Identity Consistency**
   - Advanced models capture more discriminative features
   - Reduced identity switches during occlusions
   - More robust to viewpoint and pose changes

2. **Improved Cross-Camera Tracking**
   - Better feature representation for matching across views
   - Enhanced robustness to lighting and appearance changes
   - Superior handling of similar-looking individuals

3. **Enhanced Long-term Tracking**
   - More stable tracklet features over time
   - Better recovery after long occlusions
   - Improved handling of appearance variations

### Model Training Details

All models are trained on large-scale person Re-ID datasets:

- **Market-1501**: 32,668 images, 1,501 identities
- **MSMT17**: 126,441 images, 4,101 identities (more diverse)
- **CUHK03**: Includes both labeled and detected person bounding boxes
- **DukeMTMC-reID**: Multi-camera tracking dataset

## Troubleshooting

### Common Issues

1. **Model download fails**
   ```bash
   # Try downloading manually or check internet connection
   python setup_reid_models.py --download model_name --force
   ```

2. **Out of memory errors**
   ```bash
   # Use a smaller model or reduce batch size
   python setup_reid_models.py --recommend speed
   ```

3. **Slow tracking performance**
   ```bash
   # Switch to a faster model
   python setup_reid_models.py --use osnet_x0_5_msmt17.pt
   ```

4. **Poor tracking accuracy**
   ```bash
   # Use a more accurate model
   python setup_reid_models.py --use clip_vit_l14_msmt17.pt
   ```

### Performance Tuning

1. **Adjust appearance threshold**
   - Lower values (0.2-0.3): More lenient matching, fewer ID switches
   - Higher values (0.4-0.6): Stricter matching, more accurate but may lose tracks

2. **Tune EMA alpha**
   - Higher values (0.9-0.95): More stable features, slower adaptation
   - Lower values (0.7-0.8): Faster adaptation, may be less stable

3. **Optimize for your hardware**
   - GPU: Use larger models (ResNeSt, CLIP)
   - CPU: Use OSNet or MobileNet variants
   - Mobile: Use MobileNetV2 model

## Advanced Usage

### Custom Model Integration

To add your own Re-ID model:

1. Add model info to `REID_MODELS` in `setup_reid_models.py`
2. Ensure model outputs compatible embeddings
3. Update embedding dimension in config
4. Test with your specific use case

### Multi-Model Ensemble

For research purposes, you can combine multiple models:

```python
# Pseudocode for ensemble approach
features_osnet = model_osnet(image_crop)
features_clip = model_clip(image_crop) 
combined_features = torch.cat([features_osnet, features_clip], dim=1)
```

## References

- [OSNet Paper](https://arxiv.org/abs/1905.00953) - Omni-Scale Feature Learning for Person Re-identification
- [ResNeSt Paper](https://arxiv.org/abs/2004.08955) - ResNeSt: Split-Attention Networks
- [CLIP Paper](https://arxiv.org/abs/2103.00020) - Learning Transferable Visual Models
- [BoT-SORT Paper](https://arxiv.org/abs/2206.14651) - Robust Associations Multi-Pedestrian Tracking
- [ConvNeXt Paper](https://arxiv.org/abs/2201.03545) - A ConvNet for the 2020s 